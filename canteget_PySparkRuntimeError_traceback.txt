
24/03/27 18:17:21 WARN Utils: Your hostname, tommaso-VirtualBox02 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)

24/03/27 18:17:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address

Setting default log level to "WARN".

To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).

24/03/27 18:17:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

Row(abstract='  In this contribution, I discuss some basic techniques that can be used to\nsimulate radiative transfer through porous media. As specific examples, I\nconsider scattering transfer through a clumped slab, and X-ray emission line\nformation in a clumped wind.\n', authors='Rich Townsend', authors_parsed=[['Townsend', 'Rich', '']], categories='astro-ph', comments='3 pages, 2 figures, to appear in proceedings of "Clumping in Hot Star\n  Winds" conference (Potsdam, Germany, June 2007)', doi=None, id='0709.0860', journal-ref=None, license=None, report-no=None, submitter='Richard Townsend', title='Techniques for simulating radiative transfer through porous media', update_date='2007-09-07', versions=[Row(created='Thu, 6 Sep 2007 13:14:43 GMT', version='v1')])

Row(abstract='  The properties of their hosts provide important clues to the progenitors of\ndifferent classes of gamma-ray bursts (GRBs). The hosts themselves also\nconstitute a sample of high-redshift star-forming galaxies which, unlike most\nother methods, is not selected on the luminosities of the galaxies themselves.\nWe discuss what we have learnt from and about GRB host galaxies to date.\n', authors='N. R. Tanvir and A. J. Levan', authors_parsed=[['Tanvir', 'N. R.', ''], ['Levan', 'A. J.', '']], categories='astro-ph', comments='10 pages, 4 figures, to appear in proceedings of 070228: The next\n  decade of GRB afterglows, Amsterdam March 2007', doi=None, id='0709.0861', journal-ref=None, license=None, report-no=None, submitter='Nial R. Tanvir', title='The population of GRB hosts', update_date='2007-09-07', versions=[Row(created='Thu, 6 Sep 2007 16:41:41 GMT', version='v1'), Row(created='Fri, 7 Sep 2007 09:11:10 GMT', version='v2')])







24/03/27 18:17:34 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)

org.apache.spark.api.python.PythonException: Traceback (most recent call last):

  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 812, in main

    func, profiler, deserializer, serializer = read_command(pickleSer, infile)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 87, in read_command

    command = serializer._read_with_length(file)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 174, in _read_with_length

    return self.loads(obj)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 472, in loads

    return cloudpickle.loads(obj, encoding=encoding)

AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>



	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)

	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:139)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	at java.base/java.lang.Thread.run(Thread.java:840)

24/03/27 18:17:34 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4) (10.0.2.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):

  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 812, in main

    func, profiler, deserializer, serializer = read_command(pickleSer, infile)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 87, in read_command

    command = serializer._read_with_length(file)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 174, in _read_with_length

    return self.loads(obj)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 472, in loads

    return cloudpickle.loads(obj, encoding=encoding)

AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>



	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)

	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:139)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	at java.base/java.lang.Thread.run(Thread.java:840)



24/03/27 18:17:34 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job

Traceback (most recent call last):

  File "/home/tommaso/tommaso03/coding_projects/corso_spark_kafka_flink_ntt/esercizi/esercizi_spark/RDD_Esercizio_NoSol_v2.py", line 42, in <module>

    unique_licenses = rdd.map(lambda x: x["license"]).distinct().collect()

  File "/home/tommaso/.local/lib/python3.10/site-packages/pyspark/rdd.py", line 1833, in collect

    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())

  File "/home/tommaso/.local/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__

    return_value = get_return_value(

  File "/home/tommaso/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco

    return f(*a, **kw)

  File "/home/tommaso/.local/lib/python3.10/site-packages/py4j/protocol.py", line 326, in get_return_value

    raise Py4JJavaError(

py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.

: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (10.0.2.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):

  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 812, in main

    func, profiler, deserializer, serializer = read_command(pickleSer, infile)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 87, in read_command

    command = serializer._read_with_length(file)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 174, in _read_with_length

    return self.loads(obj)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 472, in loads

    return cloudpickle.loads(obj, encoding=encoding)

AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>



	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)

	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:139)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	at java.base/java.lang.Thread.run(Thread.java:840)



Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)

	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)

	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)

	at org.apache.spark.rdd.RDD.collect(RDD.scala:1018)

	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)

	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:568)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:840)

Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):

  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 812, in main

    func, profiler, deserializer, serializer = read_command(pickleSer, infile)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 87, in read_command

    command = serializer._read_with_length(file)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 174, in _read_with_length

    return self.loads(obj)

  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 472, in loads

    return cloudpickle.loads(obj, encoding=encoding)

AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>



	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)

	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)

	at org.apache.spark.scheduler.Task.run(Task.scala:139)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	... 1 more

